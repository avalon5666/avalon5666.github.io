---
title: MySQL Router 源码学习-线程模型
date: 2023-12-02T15:58:15.000Z
updated: 2023-12-19T16:14:06.000Z
categories: MySQL
---

## 线程模型

![](/images/1f87088eaf85a5c266159f1d3dd1bf1f.jpeg)

Router 的线程模是 Reactor 多线程。Linux 网络编程大部分都是这种模型，固定的线程就可以处理大量连接。

1. io_main 线程负责处理客户端建立连接请求。
2. io_0~N 线程负责处理客户端和 MySQL 连接的读写请求。
3. 多个路由都是同一组 IO 线程来处理。

底层封装了操作系统的 poll、epoll、kqueue，linux 默认是 epoll，mac 默认是 poll。

### 插件

各种组件都定义成插件，定义了 init 和 start 等方法，主线程启动的时候调用所有插件的 init 和 start 方法。这些插件包括：logger、io、connection_pool、destination_status、router_openssl、routing

```
std::exception_ptr Loader::run() {
  std::exception_ptr first_eptr = init_all();
  if (!first_eptr) {
    try {
      start_all();
    }
  }
}

std::exception_ptr Loader::init_all() {
  for (auto it = order_.begin(); it != order_.end(); ++it) {
    call_plugin_function(&env, eptr, info.plugin()->init, "init",
                         plugin_name.c_str());
  }
}

void Loader::start_all() {
  try {
    for (const ConfigSection *section : config_.sections()) {
      call_plugin_function(this_thread_env.get(), eptr, fptr, "start",
                             section->name.c_str(), section->key.c_str());
	}
}
```

```
extern "C" {
mysql_harness::Plugin IO_EXPORT harness_plugin_io = {
    mysql_harness::PLUGIN_ABI_VERSION,       // abi-version
    mysql_harness::ARCHITECTURE_DESCRIPTOR,  // arch-descriptor
    "IO",
    VERSION_NUMBER(0, 0, 1),
    // requires
    required.size(),
    required.data(),
    // conflicts
    0,
    nullptr,
    init,     // init
    deinit,   // deinit
    run,      // run
    nullptr,  // on_signal_stop
    false,    // signals ready
    supported_options.size(),
    supported_options.data(),
};
}
```

### io 插件

io 插件 init 和 start 主要是执行 IoComponent 执行 init 和 start。IoComponent 是个单例设计，IoComponent 的 io_ctx 负责处理客户端建立连接请求，io_threads_ 是处理连接的读写请求的线程，每个线程绑定一个 io_ctx_。io_main 线程在 start 阶段启动，io_1～N 线程在 init 阶段就会启动。

```
IoComponent &IoComponent::get_instance() {
  static IoComponent instance;
  return instance;
}

class IO_COMPONENT_EXPORT IoComponent {
  std::list<IoThread> io_threads_;
  std::unique_ptr<net::io_context> io_ctx_;
}
```

```
stdx::expected<void, std::error_code> IoComponent::init(
    size_t num_worker_threads, const std::string &backend_name) {
  io_ctx_ = std::make_unique<net::io_context>(
      std::make_unique<net::impl::socket::SocketService>(),
      IoBackend::backend(backend_name));
  for (size_t ndx{}; ndx < num_worker_threads; ++ndx) {
    try {
      io_threads_.emplace_back(ndx, cpu_affinity, backend_name);
    }
}

IoThread(size_t ndx, std::bitset<ThreadAffinity::max_cpus> cpu_affinity,
           const std::string &backend_name = "poll")
      : ndx_{ndx},
        cpu_affinity_{cpu_affinity},
        io_ctx_{std::make_unique<net::impl::socket::SocketService>(),
                IoBackend::backend(backend_name)},
        thr_{&IoThread::operator(), this} {}

void IoThread::operator()() {
  my_thread_self_setname(("io-"s + std::to_string(ndx_)).c_str());
  io_ctx_.run();
}
```

```
static void run(mysql_harness::PluginFuncEnv * /* env */) {
  my_thread_self_setname("io_main");
  IoComponent::get_instance().run();
}

void IoComponent::run() {
  if (io_ctx_) io_ctx_->run();
}
```

io_main，io_N 线程启动后的核心逻辑是 io_context run 方法，循环处理各类任务、事件，通过函数指针执行对应的业务逻辑，分为四类，它们的优先级从高到低：

1. 定时任务
2. 延迟任务
3. 取消任务
4. 网络事件

这个逻辑里面涉及多个 mutex，从代码来看 io_context 只会在对应的线程执行，按道理不需要加锁，这里确实没搞明白。简单压测，这些锁看起来没有造成性能瓶颈。

```
inline io_context::count_type io_context::run() {
  for (wait_no_runner_unlocked_(lk); do_one(lk, -1ms) != 0;
       wait_no_runner_(lk)) {
  }
}

inline io_context::count_type io_context::do_one(
    std::unique_lock<std::mutex> &lk, std::chrono::milliseconds timeout) {
  while (true) {
    if (timer_q) {
      // 定时任务
      if (timer_q->run_one()) {
        return 1;
      }
    }

  	// 延迟任务
    if (deferred_work_.run_one()) {
      wake_one_runner_(lk);
      return 1;
    }

  	// 取消任务
		if (auto op = [this]() -> std::unique_ptr<async_op> {
          if (!cancelled_ops_.empty() &&
              cancelled_ops_.front()->is_cancelled()) {
            auto op = std::move(cancelled_ops_.front());
            return op;
          }
        }()) {
      op->run(*this);
    }

    auto res = io_service_->poll_one(min_duration);
    if (auto op = [this](native_handle_type fd,
                         short events) -> std::unique_ptr<async_op> {
          return active_ops_.extract_first(fd, events);
        }(res->fd, res->event)) {
      // 网络事件
      op->run(*this);
    }
  }
}
```

定时任务、延迟任务、取消任务的实现都比较简单，主要是一些数据结构的运用，这里就不展开说。
网络事件处理分为两部分：

1. IoServiceBase 是对 IO 复用库的封装，对上层提供统一的接口，实现类有 poll_io_service、linux_epoll_io_service、kqueue_io_service。
2. AsyncOps，保存网络事件和触发后所调用的业务逻辑函数指针。

网络事件的注册最后会调用 io_context async_wait 方法，accept 和 read 的 wait_type 都是 wait_read，它们都直接使用 IO 复用的方法，write 当 socket 是非阻塞 IO，就使用延迟任务，否则使用 IO 复用的方法。
当触发网络事件后，会根据 fd 和 events 在 active_ops_ 查找对应的业务逻辑函数指针并执行。

```
 void async_wait(native_handle_type fd, impl::socket::wait_type wt, Op &&op) {
    active_ops_.push_back(
        std::make_unique<async_op_impl<Op>>(std::forward<Op>(op), fd, wt));
    {
      auto res = io_service_->add_fd_interest(fd, wt);
    }
  }
```

```
void AcceptingEndpointTcpSocket::start(MySQLRouting *r,
                                       std::list<IoThread> &io_threads,
                                       WaitableMonitor<Nothing> &waitable) {
  if (service_.is_open()) {
    service_.native_non_blocking(true);
    service_.async_wait(net::socket_base::wait_read,
                        Acceptor<net::ip::tcp>(r, io_threads, service_,
                                               service_endpoint_, waitable));
  }
}
```

```
void async_recv(recv_buffer_type &buf,
                std::function<void(std::error_code ec, size_t transferred)>
                    completion) override {
  net::async_read(sock_, net::dynamic_buffer(buf), net::transfer_at_least(1),
                  std::move(completion));
}

template <class AsyncReadStream, class DynamicBuffer, class CompletionCondition,
          class CompletionToken>
std::enable_if_t<is_dynamic_buffer<DynamicBuffer>::value, void> async_read(
    AsyncReadStream &stream, DynamicBuffer &&b,
    CompletionCondition completion_condition, CompletionToken &&token) {
  async_completion<CompletionToken, void(std::error_code, size_t)> init{token};

  using compl_handler_type = typename decltype(init)::completion_handler_type;

  class Completor {
    void operator()(std::error_code ec) {
      const auto res = net::read(stream_, b_, compl_cond_);
    }

		if (!res) {
      compl_handler_(res.error(), 0);
    } else {
      compl_handler_({}, res.value());
    }
  };

  stream.async_wait(
      net::impl::socket::wait_type::wait_read,
      Completor(stream, std::forward<DynamicBuffer>(b), completion_condition,
                std::move(init.completion_handler)));
}
```

```
void async_send(recv_buffer_type &buf,
                  std::function<void(std::error_code ec, size_t transferred)>
                      completion) override {
    if (sock_.native_non_blocking()) {
      auto write_res = net::write(sock_, net::dynamic_buffer(buf),
                                  net::transfer_at_least(1));
      if (write_res) {
        net::defer(sock_.get_executor(), [completion = std::move(completion),
                                          transferred = *write_res]() {
        });
      }
    }

    net::async_write(sock_, net::dynamic_buffer(buf), net::transfer_at_least(1),
                     std::move(completion));
  }
```
### routing 插件

初始化各个路由，并且注册 accept 网络事件。触发网络事件后，创建 MysqlRoutingClassicConnection 对象，然后往连接绑定的 io_context 延迟队列扔任务。

```
void operator()(std::error_code ec) {
  waitable_([this, ec](auto &) {
    while (r_->is_running()) {
      auto sock_res = acceptor_socket_.accept(cur_io_thread_->context(),
                                              client_endpoint, socket_flags);
      if (sock_res) {
        // 绑定 io 线程，轮训
        cur_io_thread_ = std::next(cur_io_thread_);

        if (cur_io_thread_ == io_threads_.end()) {
          cur_io_thread_ = io_threads_.begin();
        }

        if (r_->get_context().blocked_endpoints().is_blocked(
                client_endpoint)) {
        } else {
          if (max_route_connections_limit_reached ||
              max_total_connections_limit_reached) {
          } else {
            r_->create_connection<client_protocol_type>(std::move(*sock),
                                                        client_endpoint);
          }
        }
      }
    }
  });
}

void MySQLRouting::create_connection(
    typename ClientProtocol::socket client_socket,
    const typename ClientProtocol::endpoint &client_endpoint) {
  net::io_context &io_ctx = client_socket.get_executor().context();

  switch (context_.get_protocol()) {
    case BaseProtocol::Type::kClassicProtocol: {
      auto new_connection = MysqlRoutingClassicConnection::create(
          context_, destinations(),
          std::make_unique<BasicConnection<ClientProtocol>>(
              std::move(client_socket), client_endpoint),
          std::make_unique<RoutingConnection<ClientProtocol>>(client_endpoint),
          remove_callback);
      
			auto *new_conn_ptr = new_connection.get();
      
			net::defer(io_ctx, [new_conn_ptr]() { new_conn_ptr->async_run(); });
    } break;
  }
}
```

## 业务模型

无论是 accept、read、write 的回调函都会走到 MysqlRoutingClassicConnection call_next_function。它的核心是通过先进先出的栈模拟函数调用 + 状态机，这样处理流程随时可以挂起和恢复，将流程异步化了。另外的实现方法有异步编程 async/await。

1. 将各种业务处理流程定义成一个个类，平常各个业务流程之间的函数调用变成用代码来模拟。各个业务流程间的耦合没了，但是代码阅读起来不太方便。
2. 业务流程的返回值会决定处理流程如何扭转。
   1. Result::Done，代表业务处理流程已经处理完毕，将对应的对象弹出栈。
   2. Result::RecvFromClient、Result::RecvFromServer 等网络等待，代表等待网络读写事件，当前的处理流程挂起，收到网络事件后再恢复执行。具体机制可以看 io 插件网络事件的处理逻辑。
   3. Again，重新执行。
   4. Result::Suspend、Result::Void，挂起。
3. 各个业务处理流程内部使用状态机，每个步骤对应一个函数，挂起恢复后可以重入。

```
void MysqlRoutingClassicConnection::async_run() {
  this->accepted();

	// 往栈推入处理流程对象
  push_processor(std::make_unique<FlowProcessor>(this));

  call_next_function(Function::kLoop);
}
```

```
void call_next_function(Function next) {
  switch (next) {
    case Function::kLoop:
      return loop();
  }
}

void MysqlRoutingClassicConnectionBase::loop() {
  while (!processors_.empty()) {
    auto res = processors_.back()->process();
    switch (*res) {
      case Processor::Result::Done:
        processors_.pop_back();
        break;
      case Processor::Result::RecvFromClient:
        return async_recv_client(Function::kLoop);
      case Processor::Result::RecvFromServer:
        return async_recv_server(Function::kLoop);
      case Processor::Result::RecvFromBoth:
        async_recv_client(Function::kLoop);
        async_recv_server(Function::kLoop);
        return;
      case Processor::Result::SendToClient:
        return async_send_client(Function::kLoop);
      case Processor::Result::SendToServer:
        return async_send_server(Function::kLoop);
      case Processor::Result::SendableToServer:
        return async_wait_send_server(Function::kLoop);
      case Processor::Result::Again:
        break;
      case Processor::Result::Suspend:
      case Processor::Result::Void:
        return;
    }
  }
}

void MysqlRoutingClassicConnectionBase::async_recv_client(Function next) {
  this->socket_splicer()->async_recv_client([this, next](std::error_code ec,
                                                         size_t transferred) {
    return trace_and_call_function(Tracer::Event::Direction::kClientToRouter,
                                   "io::recv", next);
  });
}

void MysqlRoutingClassicConnectionBase::trace_and_call_function(
    Tracer::Event::Direction dir, std::string_view stage,
    MysqlRoutingClassicConnectionBase::Function func) {
  call_next_function(func);
}

```

```
class FlowProcessor : public Processor {
 public
  enum class Stage {
    Greeting,
    Command,

    Done,
  };

  stdx::expected<Processor::Result, std::error_code> process() override;

 private:
  stdx::expected<Processor::Result, std::error_code> greeting();
  stdx::expected<Processor::Result, std::error_code> command();
};

stdx::expected<Processor::Result, std::error_code> FlowProcessor::process() {
  switch (stage()) {
    case Stage::Greeting:
      return greeting();
    case Stage::Command:
      return command();
    case Stage::Done:
      return Result::Done;
  }
}

stdx::expected<Processor::Result, std::error_code> FlowProcessor::greeting() {
  stage(Stage::Command);

  connection()->push_processor(std::make_unique<ClientGreetor>(connection()));
}
```

整体处理流程如下：

![](/images/6e2e556fc4a52d0491630b94b7ee65c8.jpeg)

